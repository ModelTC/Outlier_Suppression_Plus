quant:
    a_qconfig:
        quantizer: GroupFixedFakeQuantize
        group_size: 1024
        observer: MinMaxObserver # EMAMSEObserver EMAMinMaxObserver EMAQuantileObserver EMAPruneMinMaxObserver
        bit: 4
        symmetric: False
        ch_axis: 0
    w_qconfig:
        quantizer: GroupFixedQuantize
        group_size: 1024
        observer: MinMaxObserver
        bit: 4
        symmetric: False
        ch_axis: 0 # perchannel 0 perlayer -1
    calibrate: 128
    calibrate_path: /mnt/lustre/weixiuying.vendor/datasets/nlp_datasets/pile_cali    
    is_remove_padding: True
    except_quantizer: [query_permute_post_act_fake_quant, key_transpose_post_act_fake_quant, value_permute_post_act_fake_quant, attention_probs_post_act_fake_quant] # disable bmm quantization
    migrate: True
model:
    max_length: 2048
